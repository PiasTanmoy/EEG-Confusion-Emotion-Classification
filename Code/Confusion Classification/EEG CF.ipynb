{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EEG CF.ipynb","provenance":[],"authorship_tag":"ABX9TyNGCHdAdZvIuYpduxR1jWPN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SogOf2HmHIar"},"source":["Vanilla CNN"]},{"cell_type":"code","metadata":{"id":"6jfEYaG2Gss7","executionInfo":{"status":"ok","timestamp":1617375867126,"user_tz":240,"elapsed":2555,"user":{"displayName":"Tanmoy Sarkar Pias","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw2jBKYHMQQV4HWs5VXZPywA2hT3Bv9Oz07bJ2=s64","userId":"17617268413497684369"}}},"source":["import numpy as np\n","import tensorflow as tf\n","\n","def weight_variable(shape):\n","    initializer = tf.truncated_normal_initializer(dtype=tf.float32, stddev=1e-1)\n","    return tf.get_variable(\"weights\", shape,initializer=initializer, dtype=tf.float32)\n","\n","def bias_variable(shape):\n","    initializer = tf.constant_initializer(0.0)\n","    return tf.get_variable(\"biases\", shape, initializer=initializer, dtype=tf.float32)\n","\n","def conv2d(x, W):\n","    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n","\n","def max_pool_2x2(x):\n","    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","class MNISTcnn(object):\n","    def __init__(self, x, y, conf):\n","        self.x = tf.reshape(x, shape=[-1, 28, 28, 1])\n","        self.y = y\n","\n","        # conv1\n","        with tf.variable_scope('conv1'):\n","            W_conv1 = weight_variable([5, 5, 1, 32])\n","            b_conv1 = bias_variable([32])\n","            h_conv1 = tf.nn.relu(conv2d(self.x, W_conv1) + b_conv1)\n","            h_pool1 = max_pool_2x2(h_conv1)\n","\n","        # conv2\n","        with tf.variable_scope('conv2'):\n","            W_conv2 = weight_variable([5, 5, 32, 64])\n","            b_conv2 = bias_variable([64])\n","            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n","            h_pool2 = max_pool_2x2(h_conv2)\n","\n","        # fc1\n","        with tf.variable_scope(\"fc1\"):\n","            shape = int(np.prod(h_pool2.get_shape()[1:]))\n","            W_fc1 = weight_variable([shape, 1024])\n","            b_fc1 = bias_variable([1024])\n","            h_pool2_flat = tf.reshape(h_pool2, [-1, shape])\n","            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n","\n","\n","        # fc2\n","        with tf.variable_scope(\"fc2\"):\n","            W_fc2 = weight_variable([1024, 10])\n","            b_fc2 = bias_variable([10])\n","            y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n","\n","        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=y_conv))\n","        self.pred = tf.argmax(y_conv, 1)\n","\n","        self.norm = tf.norm(y_conv)\n","\n","        self.correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(self.y,1))\n","        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3diV-B5HPTn"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os\n","import sys\n","import csv\n","import time\n","import json\n","import argparse\n","import numpy as np\n","\n","sys.path.append('../')\n","\n","def loadData():\n","    '''\n","    example method for loading data\n","    X: data\n","    Y: labels\n","    train: training data\n","    val: validation data\n","    test: testing data\n","    '''\n","    Xtrain = None\n","    Ytrain = None\n","    Xval = None\n","    Yval = None\n","    Xtest = None\n","    Ytest = None\n","    return Xtrain, Ytrain, Xval, Yval, Xtest, Ytest\n","\n","import tensorflow as tf\n","\n","from vanilla.cnn import MNISTcnn\n","\n","def predict(sess, x, keep_prob, pred, Xtest, Ytest, output_file):\n","    feed_dict = {x:Xtest, keep_prob: 1.0}\n","    prediction = sess.run(pred, feed_dict=feed_dict)\n","\n","    with open(output_file, \"w\") as file:\n","        writer = csv.writer(file, delimiter = \",\")\n","        writer.writerow([\"id\",\"label\"])\n","        for i in range(len(prediction)):\n","            writer.writerow([str(i), str(prediction[i])])\n","\n","    print(\"Output prediction: {0}\". format(output_file))\n","\n","\n","def train(args, Xtrain, Ytrain, Xval, Yval, Xtest, Ytest):\n","    num_class = 10\n","\n","    x = tf.placeholder(tf.float32, (None, 28*28))\n","    y = tf.placeholder(tf.float32, (None, num_class))\n","    model = MNISTcnn(x, y, args)\n","\n","    optimizer = tf.train.AdamOptimizer(1e-5).minimize(model.loss)\n","\n","    saver = tf.train.Saver(tf.trainable_variables())\n","\n","    with tf.Session() as sess:\n","        print('Starting training')\n","        sess.run(tf.global_variables_initializer())\n","        if args.load_params:\n","            ckpt_file = os.path.join(args.ckpt_dir, 'mnist_model.ckpt')\n","            print('Restoring parameters from', ckpt_file)\n","            saver.restore(sess, ckpt_file)\n","\n","        num_batches = Xtrain.shape[0] // args.batch_size\n","       \n","        validation = True\n","        val_num_batches = Xval.shape[0] // args.batch_size\n","\n","        test_num_batches = Xtest.shape[0] // args.batch_size\n","\n","        best_validate_accuracy = 0\n","        score = 0\n","\n","        for epoch in range(args.epochs):\n","            begin = time.time()\n","\n","            # train\n","            train_accuracies = []\n","            for i in range(num_batches):\n","\n","                batch_x = Xtrain[i*args.batch_size:(i+1)*args.batch_size,:]\n","                batch_y = Ytrain[i*args.batch_size:(i+1)*args.batch_size,:]\n","\n","                _, acc = sess.run([optimizer, model.accuracy], feed_dict={x: batch_x, y: batch_y})\n","                train_accuracies.append(acc)\n","            train_acc_mean = np.mean(train_accuracies)\n","\n","            # compute loss over validation data\n","            if validation:\n","                val_accuracies = []\n","                for i in range(val_num_batches):\n","                    batch_x = Xval[i*args.batch_size:(i+1)*args.batch_size,:]\n","                    batch_y = Yval[i*args.batch_size:(i+1)*args.batch_size,:]\n","                    acc = sess.run(model.accuracy, feed_dict={x: batch_x, y: batch_y})\n","                    val_accuracies.append(acc)\n","                val_acc_mean = np.mean(val_accuracies)\n","\n","                # log progress to console\n","                print(\"Epoch %d, time = %ds, train accuracy = %.4f, validation accuracy = %.4f\" % (epoch, time.time()-begin, train_acc_mean, val_acc_mean))\n","            else:\n","                print(\"Epoch %d, time = %ds, train accuracy = %.4f\" % (epoch, time.time()-begin, train_acc_mean))\n","            sys.stdout.flush()\n","\n","            if val_acc_mean > best_validate_accuracy:\n","                best_validate_accuracy = val_acc_mean\n","\n","                test_accuracies = []\n","                for i in range(test_num_batches):\n","                    batch_x = Xtest[i*args.batch_size:(i+1)*args.batch_size,:]\n","                    batch_y = Ytest[i*args.batch_size:(i+1)*args.batch_size,:]\n","                    acc = sess.run(model.accuracy, feed_dict={x: batch_x, y: batch_y})\n","                    test_accuracies.append(acc)\n","                score = np.mean(test_accuracies)\n","\n","                print(\"Best Validated Model Prediction Accuracy = %.4f \" % (score))\n","\n","            if (epoch + 1) % 10 == 0:\n","                ckpt_file = os.path.join(args.ckpt_dir, 'mnist_model.ckpt')\n","                saver.save(sess, ckpt_file)\n","\n","        ckpt_file = os.path.join(args.ckpt_dir, 'mnist_model.ckpt')\n","        saver.save(sess, ckpt_file)\n","\n","        print(\"Best Validated Model Prediction Accuracy = %.4f \" % (score))\n","\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-c', '--ckpt_dir', type=str, default='ckpts/', help='Directory for parameter checkpoints')\n","    parser.add_argument('-l', '--load_params', dest='load_params', action='store_true', help='Restore training from previous model checkpoint?')\n","    parser.add_argument(\"-o\", \"--output\",  type=str, default='prediction.csv', help='Prediction filepath')\n","    parser.add_argument('-e', '--epochs', type=int, default=250, help='How many epochs to run in total?')\n","    parser.add_argument('-b', '--batch_size', type=int, default=128, help='Batch size during training per GPU')\n","    parser.add_argument('-s', '--seed', type=int, default=0, help='random seed for generating data')\n","    args = parser.parse_args()\n","\n","    if not os.path.exists(args.ckpt_dir):\n","        os.makedirs(args.ckpt_dir)\n","\n","    Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = loadData()\n","    tf.set_random_seed(100)\n","    np.random.seed(100)\n","    train(args, Xtrain, Ytrain, Xval, Yval, Xtest, Ytest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0GH4uP4Ia17"},"source":["**CF CNN**"]},{"cell_type":"code","metadata":{"id":"9ho5tJ6vISPw"},"source":["import numpy as np\n","import tensorflow as tf\n","\n","def weight_variable(shape):\n","    initializer = tf.truncated_normal_initializer(dtype=tf.float32, stddev=1e-1)\n","    return tf.get_variable(\"weights\", shape,initializer=initializer, dtype=tf.float32)\n","\n","def bias_variable(shape):\n","    initializer = tf.constant_initializer(0.0)\n","    return tf.get_variable(\"biases\", shape, initializer=initializer, dtype=tf.float32)\n","\n","def conv2d(x, W):\n","    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n","\n","def max_pool_2x2(x):\n","    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","class MNISTcnn(object):\n","    def __init__(self, x, y, conf):\n","        self.x = tf.reshape(x, shape=[-1, 28, 28, 1])\n","        self.y = y\n","\n","        # conv1\n","        with tf.variable_scope('conv1'):\n","            W_conv1 = weight_variable([5, 5, 1, 32])\n","            b_conv1 = bias_variable([32])\n","            h_conv1 = tf.nn.relu(conv2d(self.x, W_conv1) + b_conv1)\n","            h_pool1 = max_pool_2x2(h_conv1)\n","\n","        # conv2\n","        with tf.variable_scope('conv2'):\n","            W_conv2 = weight_variable([5, 5, 32, 64])\n","            b_conv2 = bias_variable([64])\n","            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n","            h_pool2 = max_pool_2x2(h_conv2)\n","\n","        # fc1\n","        with tf.variable_scope(\"fc1\"):\n","            shape = int(np.prod(h_pool2.get_shape()[1:]))\n","            W_fc1 = weight_variable([shape, 1024])\n","            b_fc1 = bias_variable([1024])\n","            h_pool2_flat = tf.reshape(h_pool2, [-1, shape])\n","            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n","\n","\n","        # fc2\n","        with tf.variable_scope(\"fc2\"):\n","            W_fc2 = weight_variable([1024, 10])\n","            b_fc2 = bias_variable([10])\n","            y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n","\n","        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=y_conv))\n","        self.pred = tf.argmax(y_conv, 1)\n","\n","        self.norm = tf.norm(y_conv)\n","\n","        self.correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(self.y,1))\n","        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n","\n","        self.layer = W_fc1 # because we are interested in adjusting the weights of W_fc1\n","\n","    def setWeights(self, session, weights):\n","        for v in tf.trainable_variables():\n","            session.run(v.assign(weights[v.name]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6zXGfktIg3Q"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os\n","import sys\n","import csv\n","import time\n","import json\n","import argparse\n","import numpy as np\n","\n","sys.path.append('../')\n","\n","def loadData():\n","    '''\n","    example method for loading data\n","    X: data\n","    Y: labels\n","    train: training data\n","    val: validation data\n","    test: testing data\n","    '''\n","    Xtrain = None\n","    Ytrain = None\n","    Xval = None\n","    Yval = None\n","    Xtest = None\n","    Ytest = None\n","    return Xtrain, Ytrain, Xval, Yval, Xtest, Ytest\n","\n","def loadCFLabels():\n","    '''\n","    corresponding labels of confounding factors\n","    The method does not really need Zval or Ztest\n","    '''\n","    Ztrain = None\n","    Zval = None\n","    Ztest = None\n","    return Ztrain, Zval, Ztest\n","\n","import tensorflow as tf\n","\n","from CF.cnn import MNISTcnn\n","#from tensorflow.examples.tutorials.mnist import input_data\n","\n","def predict(sess, x, keep_prob, pred, Xtest, Ytest, output_file):\n","    feed_dict = {x:Xtest, keep_prob: 1.0}\n","    prediction = sess.run(pred, feed_dict=feed_dict)\n","\n","    with open(output_file, \"w\") as file:\n","        writer = csv.writer(file, delimiter = \",\")\n","        writer.writerow([\"id\",\"label\"])\n","        for i in range(len(prediction)):\n","            writer.writerow([str(i), str(prediction[i])])\n","\n","    print(\"Output prediction: {0}\". format(output_file))\n","\n","\n","def train(args, Xtrain, Ytrain, Xval, Yval, Xtest, Ytest, Ztrain):\n","    num_class = 10\n","\n","    x = tf.placeholder(tf.float32, (None, 28*28))\n","    y = tf.placeholder(tf.float32, (None, num_class))\n","    model = MNISTcnn(x, y, args)\n","\n","    optimizer = tf.train.AdamOptimizer(1e-5).minimize(model.loss)\n","\n","    saver = tf.train.Saver(tf.trainable_variables())\n","\n","    weights = {}\n","\n","    with tf.Session() as sess:\n","        print('Starting training')\n","        sess.run(tf.global_variables_initializer())\n","        if args.load_params:\n","            ckpt_file = os.path.join(args.ckpt_dir, 'mnist_model.ckpt')\n","            print('Restoring parameters from', ckpt_file)\n","            saver.restore(sess, ckpt_file)\n","\n","        num_batches = Xtrain.shape[0] // args.batch_size\n","       \n","        validation = True\n","        val_num_batches = Xval.shape[0] // args.batch_size\n","\n","        test_num_batches = Xtest.shape[0] // args.batch_size\n","\n","        best_validate_accuracy = 0\n","        score = 0\n","\n","        # Phase One\n","        for epoch in range(args.epochs):\n","            begin = time.time()\n","\n","            # train\n","            train_accuracies = []\n","            for i in range(num_batches):\n","\n","                batch_x = Xtrain[i*args.batch_size:(i+1)*args.batch_size,:]\n","                batch_y = Ytrain[i*args.batch_size:(i+1)*args.batch_size,:]\n","\n","                _, acc = sess.run([optimizer, model.accuracy], feed_dict={x: batch_x, y: batch_y})\n","                train_accuracies.append(acc)\n","            train_acc_mean = np.mean(train_accuracies)\n","\n","            # compute loss over validation data\n","            if validation:\n","                val_accuracies = []\n","                for i in range(val_num_batches):\n","                    batch_x = Xval[i*args.batch_size:(i+1)*args.batch_size,:]\n","                    batch_y = Yval[i*args.batch_size:(i+1)*args.batch_size,:]\n","                    acc = sess.run(model.accuracy, feed_dict={x: batch_x, y: batch_y})\n","                    val_accuracies.append(acc)\n","                val_acc_mean = np.mean(val_accuracies)\n","\n","                # log progress to console\n","                print(\"Epoch %d, time = %ds, train accuracy = %.4f, validation accuracy = %.4f\" % (\n","                epoch, time.time() - begin, train_acc_mean, val_acc_mean))\n","            else:\n","                print(\"Epoch %d, time = %ds, train accuracy = %.4f\" % (epoch, time.time() - begin, train_acc_mean))\n","            sys.stdout.flush()\n","\n","            if val_acc_mean > best_validate_accuracy:\n","                best_validate_accuracy = val_acc_mean\n","\n","                test_accuracies = []\n","                for i in range(test_num_batches):\n","                    batch_x = Xtest[i*args.batch_size:(i+1)*args.batch_size,:]\n","                    batch_y = Ytest[i*args.batch_size:(i+1)*args.batch_size,:]\n","                    acc = sess.run(model.accuracy, feed_dict={x: batch_x, y: batch_y})\n","                    test_accuracies.append(acc)\n","                score = np.mean(test_accuracies)\n","\n","                print(\"Best Validated Model Prediction Accuracy = %.4f \" % (score))\n","\n","            if (epoch + 1) % 10 == 0:\n","                ckpt_file = os.path.join(args.ckpt_dir, 'mnist_model.ckpt')\n","                saver.save(sess, ckpt_file)\n","\n","        for v in tf.trainable_variables():\n","            weights[v.name] = v.eval()\n","\n","        # Phase Two\n","        weight_pre = weights[args.layer]\n","        changes = np.zeros_like(weights)\n","        for epoch in range(args.epochs_cf):\n","            begin = time.time()\n","\n","            # train\n","            for i in range(num_batches):\n","\n","                batch_x = Xtrain[i*args.batch_size:(i+1)*args.batch_size,:]\n","                batch_z = Ztrain[i*args.batch_size:(i+1)*args.batch_size,:]\n","\n","                _, acc, weight = sess.run([optimizer, model.accuracy, model.layer], feed_dict={x: batch_x, y: batch_z})\n","                changes += np.abs(weight - weight_pre)/np.max(np.abs(weight - weight_pre))\n","                weight_pre = weight\n","        changes = changes/(args.epochs_cf*num_batches)\n","\n","\n","        # Phase Three\n","        weights[args.layer][changes>args.threshold] = 0\n","        model.setWeights(sess, weights)\n","\n","        ckpt_file = os.path.join(args.ckpt_dir, 'mnist_model.ckpt')\n","        saver.save(sess, ckpt_file)\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-c', '--ckpt_dir', type=str, default='ckpts/', help='Directory for parameter checkpoints')\n","    parser.add_argument('-l', '--load_params', dest='load_params', action='store_true', help='Restore training from previous model checkpoint?')\n","    parser.add_argument(\"-o\", \"--output\",  type=str, default='prediction.csv', help='Prediction filepath')\n","    parser.add_argument('-e', '--epochs', type=int, default=250, help='How many epochs to run in first phase?')\n","    parser.add_argument('-p', '--epochs_cf', type=int, default=50, help='How many epochs to run in second phase?')\n","    parser.add_argument('-b', '--batch_size', type=int, default=128, help='Batch size during training per GPU')\n","    parser.add_argument('-t', '--threshold', type=int, default=0.75, help='threshold of updating the weights')\n","    parser.add_argument('-n', '--layer', type=str, default='fc1_weights', help='the layer we are interested in adjust')\n","    parser.add_argument('-s', '--seed', type=int, default=0, help='random seed for generating data')\n","    args = parser.parse_args()\n","\n","    if not os.path.exists(args.ckpt_dir):\n","        os.makedirs(args.ckpt_dir)\n","\n","    Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = loadData()\n","    Ztrain, Zval, Ztest = loadCFLabels()\n","    tf.set_random_seed(100)\n","    np.random.seed(100)\n","    train(args, Xtrain, Ytrain, Xval, Yval, Xtest, Ytest, Ztrain)"],"execution_count":null,"outputs":[]}]}